%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Template: Project Titlepage Modified (v 0.1) by rcx
%
% Original Source: http://www.howtotex.com
% Date: February 2014
% 
% This is a title page template which be used for articles & reports.
% 
% This is the modified version of the original Latex template from
% aforementioned website.
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[paper=a4, french, 11pt]{scrartcl} 
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{subcaption}


\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

%-------------------------------------------------------------------------------
% HEADER & FOOTER
%-------------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyhead[L]{Text detection and recognition}
\fancyhead[R]{ECOLE POLYTECHNIQUE}
\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------

\begin{document}

\title{ \normalsize \textsc{Projet 3A}
        \\ [2.0cm]
        \HRule{0.5pt} \\
        \LARGE \textbf{\uppercase{Text detection and recognition}}
        \HRule{2pt} \\ [0.5cm]
        \normalsize \today \vspace*{5\baselineskip}}

\date{}

\author{
        Zhixing CAO, Yuesong SHEN \\
        Ecole Polytechnique }

\maketitle
\newpage
\tableofcontents
\newpage

%-------------------------------------------------------------------------------
% Section title formatting
\sectionfont{\scshape}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
% BODY
%-------------------------------------------------------------------------------


\section{Introduction} \mbox{} \vspace{-0.5cm}

The development of smartphones and the growing demands in content-based image understanding have made the text detection a crucial topic in machine-human interaction. It has been shown that the performance of image retrieval depends critically on the performance of text detection and recognition. For example, two book covers with different titles but identical background prove to be considered virtually indistinguish without detecting and recognizing the text \cite{epshtein2010detecting}. A machine can distinguish these two books only by recognize their titles.

\subsection{Project definition} \mbox{} \vspace{-0.5cm}

Early approach of text detection and recognition techniques such as OCR techniques can be traced in early 1900s. Recent years, with the progress in the field of machine learning, pattern recognition and text localisation techniques make new breakthroughs \cite{anzai2012pattern}. With the study of some of those techniques, our project concern algorithms that can decode text in images.

The approach of our project can be divided into two parts --- text detection and text recognition. The detection part detect potentiel text in images and output text candidates and the recognition part translate the image of text candidates so that machine can understand.

\subsection{Work breakdown} \mbox{} \vspace{-0.5cm}

The whole project is carried out by Zhixing CAO and Yuesong SHEN. We use about one month reading articles and set our task into three independent parts: text area identification, character extraction and character recognition.

All sub-tasks require studies of related articles and implementation works. Yuesong SHEN is in charge of the text area identification part and Zhixing CAO is in charge of the character extraction and recognition part. Here is the overview of our schedule:

\begin{figure}[h]
\begin{center}
   \includegraphics[width=0.95\linewidth]{breakdowns.png}
\end{center}
\caption{Schedule}
\label{fig:schedule}
\end{figure}

\subsection{State-of-the-art} \mbox{} \vspace{-0.5cm}

\subsubsection{Text detection} \mbox{} \vspace{-0.5cm}

As an essential prerequiste for text recognization, text within images has to be robustly located. This is a challenge task due to the variety of the text form, such as variations in languages, font and style, geometric and photometric distortions, partial occlusion, and lightening conditions. Text detection problem has been studied a lot in recent researches and numerous methods are reported in the literature.

All the methods used in recent research can be classified into two catagories: method based on texture \cite{ye2003robust} \cite{kim2003texture} and method based on connected componet \cite{yin2014robust} \cite{ezaki2004text}.

Texture-based method views texts as a special texture that is distinguishable from its background. Features are extracted over some special regions and a classifier is used to identify texts areas. Zhong et al. \cite{zhong2000automatic} have segmented caption text regions from background and used the intensity variation information. Ye and al \cite{ye2003robust} have proposed a method using multiscale wavelet features and a coarse-to-fine algorithm to locate text lines under different backgrounds.

Different with the texture-based method, the connected component based approach extracts regions from the image and selects text candidates using some geometric rules. In ICDAR 2005 text locating competition \cite{lucas2005icdar}, the best result applies have used an adaptive binarization method to find connected components and forms text lines based on geometric properties. Recently Chen et al. \cite{chen2011robust} extract letter candidates by employing edge-enhanced Maximally Stable Extremal Regions and using geometric and stroke width information to exclude non-text objects. They have achieved an accuracy score of 95\%.

\subsubsection{Text recognition} \mbox{} \vspace{-0.5cm}

Converting text data from image and deciphering into digits is an important problem. Early physical photocell-based OCR implemented matrix matching by comparing an image to a stored glyph on a pixel-by-pixel basis. Those algorithms involve mostly extensive processing on the image such as thinning, smoothing contour analysis etc. because the majority of previous works uses geometrical and topological features. In recent research community the dominant approach to this problem is based on machine learning techniques --- a general inductive process building automatically a classifier by learning.

In the textbook \textit{Pattern Recognition and Machine Learning} \cite{anzai2012pattern}, Bishop reflects recent developments in the field of pattern recognition and machine learning and shows potential usages of machine learning method in pattern recognition. The early effort has been made around 1986s by Burr \cite{burr1988experiments}, Mehr\& Richfield \cite{rajavelu1989neural} to implement a neural network in character recognition. Recently, with the development of parallel computation and the use of GPU, efficient OCR system based on neural network become realistic. 

\subsection{Our approach} \mbox{} \vspace{-0.5cm}

In this report, we show you our approach of the text detection problem by combined texture-based method and connected component based method and our text recognition algorithm using neural network. 

We adapte at first the method proposed by Liu et al \cite{liu2005text} to locate texts. This algorithm detects text candidate by applied classifier on contour pictures of the original image. Experimental results demonstrate that this approach is robust for varied font-size, font-color, background and languages, which can be used efficiently.

After locating the text, a connected component based method is used to extract word candidates. We find all components by using connected-component labeling algorithm. Then we use some geometric constraints and heuristic rules to merge them and extract letters and words candidates.  

Our approach for text recognition is based on using neural network to classify characters. With a training set of different kind of characters, the neural network is constructed in order to match an input character to a learned one. 


\begin{figure}[h]
\begin{center}
	\vspace{-1ex}
   \includegraphics[width=0.9\linewidth]{process.png}
\end{center}
\vspace{-4ex}
\caption{Overview}
\label{fig:heatmap}
\end{figure}

\section{Text detection} \mbox{} \vspace{-0.5cm}

In order to extract text from an image, we first need to determine the location of text zone in order to later perform the text recognition.  

\subsection{Contour extraction} \mbox{} \vspace{-0.5cm}

Since text is always composed of strokes (so that it can be written by men), edge turns out to be an excelent characteristic for text zone identification. We have adopted the approach proposed by Liu and al. for edge information extraction, that is by applying Sobel edge detector on the image for 4 directions (vertical, horizontal, up-right to down-left and up-left to down-right) with distance defined on the RGB color space to get 4 edge maps. And this 4 edge maps are to be used to determine the text locations.

\begin{figure}[h]
\begin{center}
	\vspace{-1ex}
   \includegraphics[width=0.75\linewidth]{edge_filter_example.png}
\end{center}
\vspace{-4ex}
\caption{Edge maps}
\label{fig:heatmap}
\end{figure}

\subsection{K-means clustering} \mbox{} \vspace{-0.5cm}

With the 4 edge maps, we can then apply a sliding window of size $w \times h$ to calculate the 6 different features as follows:

\begin{itemize}
\item $\mu = \frac{1}{w \times h} \sum_{i=1}^{w} \sum_{j=1}^{h} E(i,j)$

\item $\sigma =\sqrt{\frac{1}{w \times h} \sum_{i=1}^{w} \sum_{j=1}^{h} (E(i,j) - \mu)}$

\item $ Eg = \sum_{i,j}E(i,j)^2 $

\item $ Et = \sum_{i,j} E(i,j)\log E(i,j) $

\item $ I = \sum_{i,j} (i-j)^2 E(i,j)$

\item $ H = \sum_{i,j} \frac{1}{1+(i-j)^2}E(i,j)$
\end{itemize}

Where $E(i,j)$ is the value of pixel in i\textsuperscript{th} row, j\textsuperscript{th} column, here it is the grayscale color of a pixel in an edge map. $\mu$ is the mean value in sliding window, $\sigma$ is their standard deviation, $ Eg$ is their energy, $ Et$ is their entropy, $ I$  is their gravity center and $ H$ is the expectation.

With the 4 edge maps, we then get 24 features for a given position of the sliding window. And since there is no apparent cost function available, we need an unsupervised clustering algorithm to distinguish text zones from their surroundings. We apply therefore the k-means algorithm for this, as proposed in the paper of Liu and al.

This approach, while effective for seperatings text and non-text zones, can not determine which one of the 2 zones is text zone, due to its non-supervised nature. We therefore need some extra effort to choose the text zone. There is no solution proposed in the original article. We propose 2 ideas to solve this problem: The first approach is to collect a set of manually choosed text and non-text data (an average feature vector for each zone in each image) and then train a classifier with supervised learing approach. The second approach is to use geometrical and topological informations of each zone to determine text zones.

\subsection{Text area identification} \mbox{} \vspace{-0.5cm}

The result obtained by k-means algoritm needs to be polished to remove noise and other non text zone. We first use morphology operations “open” and “dilate” to fill up tiny holes and gaps and remove too small zones in the background. We can then apply some empirical rules to further remove zones which can not contain text. Each connected component of the refined text zones will then be boxed by a rectangle and returned as final results.

\begin{figure}[h]
\begin{center}
   \includegraphics[width=0.9\linewidth]{text_area_example.png}
\end{center}
\vspace{-4ex}
\caption{Text area identification}
\label{fig:heatmap}
\end{figure}

\subsection{Connected components detection} \mbox{} \vspace{-0.5cm}

The former method shows efficient results. However, it is still incapable of distinguishing each character which is important for text recognition. So we use at next step the connected connected components algorithm to extract characters.

At first, we set the input image into a binary matrix according to a threshold. Knowing that all colors can be encoded into a grayscale double number between 0 and 255, we can transform our input image by setting all pixels to 0 or 1 with their grayscale number by a threshold. Most of the time, pixels in a single letter have similar colors, so that in the binary matrix, these pixels have always the same value which will be considered as a same component.

The connected-component labeling algorithm is based on union-find method. The first pass of the algorithm propagate a pixel's label to its eight neighbors. Whenever the situation of connectivity arises, we attribute labels and union two set if it's neccesary. At the end of the first pass, each equivalence class has been completely determined and has a unique label, which is the root of its tree in the union-find structure. A second pass through the image then performs a translation, assigning to each pixel the label of its equivalence class.

\begin{figure}[h]
\begin{center}
   \includegraphics[width=0.6\linewidth]{connected_components_example.png}
\end{center}
\vspace{-4ex}
\caption{Connected components detection}
\label{fig:heatmap}
\end{figure}

\subsection{Character extraction} \mbox{} \vspace{-0.5cm}

In many language, a single character can be composed by several different part. In the character extraction, those different components should be considered as a same part. So after the connected component detection. We calculate the gravity center $G{i}$ of each component and use gravity-color distance to centering them.

Gravity-color distance of component $ i$ and $ j$:

$\sqrt{||G{i}-G{j}||^2 + (greyscale_{i} - greyscale_{j})^2}$

What's more, as characters usually have regular forms, which is to say, the ratio between their width and their height are never too large or two small. We use the height and the width of each connected component to filter non-text cadidate as well. 

\begin{figure}[h]
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=0.75\textwidth]{connected_components.png}
\caption{Simple connected components detection}
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=0.75\textwidth]{connected_components_centering.png}
\caption{Centered connected components detection}
\end{subfigure}
\caption{Components centering example}
\label{fig:Centering}
\end{figure} 

\section{Text recognition} \mbox{} \vspace{-0.5cm}

\subsection{Neural network} \mbox{} \vspace{-0.5cm}

After extract characters, we can move to the next step and try to recognize them.

In machine learning, neural network is a powerful tool to estimate functions that maps input to output. Theoretically, it can approximate every non-linear functions when using a non-linear activation function. 

Inspired of humans' central nervous systems, neural network uses connected nodes, known as neurons, to imitate the nervous system. A neural network is a complex adaptive system with ability to \textit{learn}. It consists of multiple layers. Apart from the input layer, all layers have an activation function. By adjusting this function according to our given datas, known as training set, the network achieve to learn and understand these datas. Here is the structure of the neural network:

\begin{figure}[h]
\begin{center}
	\vspace{-1ex}
   \includegraphics[width=0.75\linewidth]{network.png}
\end{center}
\vspace{-4ex}
\caption{Structure of neural network}
\label{fig:heatmap}
\end{figure}

Our goal is to determine the activation function $\Theta$ by using gradient descent. At first, we need to initialize $\Theta$ by some random values in order to break the symmetry. Then, the forward propagation will determine the output while the backpropagation will correct the error.

In our project, we use neural network to learn digital numbers.

\subsection{Digit and letter recognition} \mbox{} \vspace{-0.5cm}

In essence, pattern recognition convers the following problem: 'Given examples of signals and the correct decisions of them, make decisions automatically with future streams'. In our project, we use the neural network to learn a set of examples of digital numbers' images. Then we use the same neural network to predict new digital numbers' images. 

For digital number recognition, we use MNIST dataset \cite{lecun1998gradient} where all digit images have been size-normalized and centered in a fixed size image of 28$\times$28 pixels. And for English letter recognition, we use the Chars74K dataset \cite{de2009character} where each letter image has a size of 1200$\times$900 pixels. For simplify our task, we resize the size of those images to 40$\times$30. Therefore, each image of digit has 784 features and each image of letter has 1200 features.

Then we put all inputs into the neural network and using the trained neural network to recognize the extracted characters from the former step.

\section{Result} \mbox{} \vspace{-0.5cm}

\subsection{Text detection} \mbox{} \vspace{-0.5cm}

Here are the text detection result with different photos:

\begin{figure}[h]
\begin{center}
   \includegraphics[width=1.0\linewidth]{results.png}
\end{center}
\vspace{-3ex}
\caption{Results on 3 images (from left to right: initial image, text area, character extraction)}
\label{fig:result}
\end{figure}

\subsection{Text recognition} \mbox{} \vspace{-0.5cm}

We show you in this subsection some results and analysis of digit recognition. The MNIST dataset consists of 70.000 handwritten digit images of 0 to 9. We have tested the network with 10.000 different images and with different parameters of the network.

\subsubsection{Influence of different parameters} \mbox{} \vspace{-0.5cm}

Here are some results with different parameters:

\begin{table}[h]
\centering
\begin{small}
	\begin{tabular}{ |c|c|c|c|c| }
	\hline
	$num\_training\_data$ & $num\_layers$ & $num\_nodes$ & $num\_iteration$ & $accuracy$ \\
	\hline
	60000 & 1 & 200  & 30 & 92.56 \\
	\hline
	6000 & 1 & 200  & 30 & 91.96 \\
	\hline
	6000 & 1 & 20  & 30 & 91.02 \\
	\hline
	6000	 & 1 & 10  & 30 & 84.59 \\
	\hline
	6000 & 2 & 20,20  & 30 & 85.7\\
	\hline
	6000 & 2 & 20,20  & 45 & 90.33\\
	\hline
	\end{tabular}
\end{small}
\end{table}

We notice that we can get fairly good result by well choosing the parameters of the neural network such as the number of layers, number of nodes, size of training data etc.. 

\subsubsection{Confusion matrix} \mbox{} \vspace{-0.5cm}

In order to better visualize the performance of the algorithm, we use here the confusion matrix to see if the system is confusing two classes. Here is the heatmap of the confusion matrix:

\begin{figure}[h]
\vspace{-1ex}
\begin{center}
   \includegraphics[width=1.0\linewidth]{heatmap_log.png}
\end{center}
\vspace{-3ex}
\caption{Heatmap of confusion matrix}
\label{fig:heatmap}
\end{figure}

Each color corresponds with $log\frac{number\_of\_predicted\_i}{number\_actual\_ i}$, with this map, we can easily determine the error caused by the confusion of numbers. For exemple, the wrong prediction of number 3 is always 5 and the wrong prediction of number 4 is always 9. That is to say, the hand-writting of 4 is similary to the hand-writting of 9 and sometimes, it may lead to confusion in our system.

\subsection{Remaining difficulties and possible improvements} \mbox{} \vspace{-0.5cm}

There are several possible improvements to our projet:

\begin{enumerate}
\item As mentioned in section 3.2, after K-means clustering, we still need to determine which cluster represents text zone and which is the text. In our project, we adopte a manual selection approach. However, if we have an image dataset large enough, we can probably develop an automatic detection of these two clusters by training all images.

\item The connected component approach can glue up multiple letters, especially when letters are in hand-written style or with a serif font. What's more, hieroglyphic symbols like Chinese characters can sometimes break into pieces. Our centering approach can solve part of these problem but can also lead to wrong conclusion. An alternative method is to apply sliding window with well defined sizes and positions in different text zones to extract characters, or in case of words written in a line, directly cut it into segments and perform extration piece by piece.

\item The approach presented in this report only works with words without to much distortion. For example, letters in graffitis which are overlapped and painted with different pattern can not be located and extracted by our system. One possible way is to using sliding window to extract every part of the image and apply text recognition to all extractions.
   
\end{enumerate}


\section{Conclusion} \mbox{} \vspace{-0.5cm}

We implement in this project a whole pipeline for content-based image understanding problem. Multiple machine learning techniques (K-means, ANN, etc.) and image processing techniques (Sobel edge detector, Ostu method, opening/closing, connected-components, etc.) have been combined in this approach for solving text detection and recognition problem.  

Text detection is an important subject for machine-human interaction and artificial intelligence with great utility. We can imagine such a system used to help visually impaired people to read books and signs in the surroundings. It can also be used for automatic cars to understand road signs and react better in real time. Thanks to recent developments in electronics, computer science and data science, efficient implementations of this technique has been made possible.  

%-------------------------------------------------------------------------------
% REFERENCES
%-------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{biblio}
\end{document}

%-------------------------------------------------------------------------------
% SNIPPETS
%-------------------------------------------------------------------------------

%\begin{figure}[!ht]
%   \centering
%   \includegraphics[width=0.8\textwidth]{file_name}
%   \caption{}
%   \centering
%   \label{label:file_name}
%\end{figure}

%\begin{figure}[!ht]
%   \centering
%   \includegraphics[width=0.8\textwidth]{graph}
%   \caption{Blood pressure ranges and associated level of hypertension (American Heart Association, 2013).}
%   \centering
%   \label{label:graph}
%\end{figure}

%\begin{wrapfigure}{r}{0.30\textwidth}
%   \vspace{-40pt}
%   \begin{center}
%       \includegraphics[width=0.29\textwidth]{file_name}
%   \end{center}
%   \vspace{-20pt}
%   \caption{}
%   \label{label:file_name}
%\end{wrapfigure}

%\begin{wrapfigure}{r}{0.45\textwidth}
%   \begin{center}
%       \includegraphics[width=0.29\textwidth]{manometer}
%   \end{center}
%   \caption{Aneroid sphygmomanometer with stethoscope (Medicalexpo, 2012).}
%   \label{label:manometer}
%\end{wrapfigure}

%\begin{table}[!ht]\footnotesize
%   \centering
%   \begin{tabular}{cccccc}
%   \toprule
%   \multicolumn{2}{c} {Pearson's correlation test} & \multicolumn{4}{c} {Independent t-test} \\
%   \midrule    
%   \multicolumn{2}{c} {Gender} & \multicolumn{2}{c} {Activity level} & \multicolumn{2}{c} {Gender} \\
%   \midrule
%   Males & Females & 1st level & 6th level & Males & Females \\
%   \midrule
%   \multicolumn{2}{c} {BMI vs. SP} & \multicolumn{2}{c} {Systolic pressure} & \multicolumn{2}{c} {Systolic Pressure} \\
%   \multicolumn{2}{c} {BMI vs. DP} & \multicolumn{2}{c} {Diastolic pressure} & \multicolumn{2}{c} {Diastolic pressure} \\
%   \multicolumn{2}{c} {BMI vs. MAP} & \multicolumn{2}{c} {MAP} & \multicolumn{2}{c} {MAP} \\
%   \multicolumn{2}{c} {W:H ratio vs. SP} & \multicolumn{2}{c} {BMI} & \multicolumn{2}{c} {BMI} \\
%   \multicolumn{2}{c} {W:H ratio vs. DP} & \multicolumn{2}{c} {W:H ratio} & \multicolumn{2}{c} {W:H ratio} \\
%   \multicolumn{2}{c} {W:H ratio vs. MAP} & \multicolumn{2}{c} {\% Body fat} & \multicolumn{2}{c} {\% Body fat} \\
%   \multicolumn{2}{c} {} & \multicolumn{2}{c} {Height} & \multicolumn{2}{c} {Height} \\
%   \multicolumn{2}{c} {} & \multicolumn{2}{c} {Weight} & \multicolumn{2}{c} {Weight} \\
%   \multicolumn{2}{c} {} & \multicolumn{2}{c} {Heart rate} & \multicolumn{2}{c} {Heart rate} \\
%   \bottomrule
%   \end{tabular}
%   \caption{Parameters that were analysed and related statistical test performed for current study. BMI - body mass index; SP - systolic pressure; DP - diastolic pressure; MAP - mean arterial pressure; W:H ratio - waist to hip ratio.}
%   \label{label:tests}
%\end{table}


\end{document}